{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e2c6e1-0186-42ff-b775-a5433d54e3f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasRegressor\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstattools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adfuller\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseasonal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seasonal_decompose\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import boxcox, invboxcox\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_prepare_data(file_path, attack_column):\n",
    "    data = pd.read_csv(file_path)\n",
    "    attack_data = data[attack_column].values\n",
    "    return attack_data\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return np.array(result)\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    level, trend = series[0], series[1] - series[0]\n",
    "    for n in range(1, len(series)):\n",
    "        value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return np.array(result)\n",
    "\n",
    "def check_stationarity(timeseries):\n",
    "    result = adfuller(timeseries)\n",
    "    return result[1] <= 0.05\n",
    "\n",
    "def preprocess_data(data, alpha=0.2, beta=0.1):\n",
    "    # Apply Box-Cox transformation\n",
    "    data_boxcox, lambda_param = boxcox(data + 1)  # Add 1 to avoid log(0)\n",
    "    \n",
    "    # Apply double exponential smoothing\n",
    "    smoothed_data = double_exponential_smoothing(data_boxcox, alpha, beta)\n",
    "    \n",
    "    # Check stationarity\n",
    "    if not check_stationarity(smoothed_data):\n",
    "        # If not stationary, take first difference\n",
    "        smoothed_data = np.diff(smoothed_data)\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(smoothed_data.reshape(-1, 1))\n",
    "    \n",
    "    return scaled_data, scaler, lambda_param\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_bayesian_lstm_model(seq_length, n_features=1):\n",
    "    model = Sequential([\n",
    "        tfp.layers.DenseVariational(64, activation='relu', input_shape=(seq_length, n_features)),\n",
    "        tfp.layers.LSTMCellReparameterization(64, recurrent_dropout=0.2),\n",
    "        tfp.layers.DenseVariational(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def create_model(seq_length, n_features=1):\n",
    "    def model():\n",
    "        return build_bayesian_lstm_model(seq_length, n_features)\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, seq_length):\n",
    "    model = KerasRegressor(build_fn=create_model(seq_length), epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Define hyperparameters to search\n",
    "    param_dist = {\n",
    "        'epochs': [50, 100, 150],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'learning_rate': [0.001, 0.0001, 0.00001]\n",
    "    }\n",
    "    \n",
    "    # Perform random search\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=1, n_jobs=-1)\n",
    "    random_search_result = random_search.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "    \n",
    "    best_model = random_search_result.best_estimator_.model\n",
    "    return best_model, random_search_result.best_params_\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, scaler, lambda_param):\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions and actual values\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Inverse Box-Cox transform\n",
    "    predictions = invboxcox(predictions, lambda_param) - 1\n",
    "    y_test = invboxcox(y_test, lambda_param) - 1\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    return predictions, mae, rmse\n",
    "\n",
    "def plot_results(actual, predicted, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual, label='Actual')\n",
    "    plt.plot(predicted, label='Predicted')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def forecast_future(model, last_sequence, n_future, scaler, lambda_param):\n",
    "    future_predictions = []\n",
    "    current_sequence = last_sequence.copy()\n",
    "    \n",
    "    for _ in range(n_future):\n",
    "        next_pred = model.predict(current_sequence.reshape(1, current_sequence.shape[0], 1))\n",
    "        future_predictions.append(next_pred[0, 0])\n",
    "        current_sequence = np.roll(current_sequence, -1)\n",
    "        current_sequence[-1] = next_pred\n",
    "    \n",
    "    future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "    future_predictions = scaler.inverse_transform(future_predictions)\n",
    "    future_predictions = invboxcox(future_predictions, lambda_param) - 1\n",
    "    \n",
    "    return future_predictions\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'FinalDataset.csv'\n",
    "    attack_column = 'DDoS-ALL'  # Change this to the desired attack type\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    raw_data = load_and_prepare_data(file_path, attack_column)\n",
    "    processed_data, scaler, lambda_param = preprocess_data(raw_data)\n",
    "    \n",
    "    # Create sequences\n",
    "    seq_length = 12  # Use 12 months of historical data to predict the next month\n",
    "    X, y = create_sequences(processed_data, seq_length)\n",
    "    \n",
    "    # Split the data\n",
    "    train_size = int(len(X) * 0.7)\n",
    "    val_size = int(len(X) * 0.15)\n",
    "    X_train, y_train = X[:train_size], y[:train_size]\n",
    "    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "    \n",
    "    # Train the model\n",
    "    model, best_params = train_model(X_train, y_train, X_val, y_val, seq_length)\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions, mae, rmse = evaluate_model(model, X_test, y_test, scaler, lambda_param)\n",
    "    \n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(y_test, predictions, f\"{attack_column} - Actual vs Predicted\")\n",
    "    \n",
    "    # Make future predictions (36 months)\n",
    "    future_months = 36\n",
    "    last_sequence = X_test[-1]\n",
    "    future_predictions = forecast_future(model, last_sequence, future_months, scaler, lambda_param)\n",
    "    \n",
    "    print(\"Predictions for the next 36 months:\")\n",
    "    for i, pred in enumerate(future_predictions):\n",
    "        print(f\"Month {i+1}: {pred[0]:.2f}\")\n",
    "    \n",
    "    # Plot future predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(y_test)), y_test, label='Historical Data')\n",
    "    plt.plot(range(len(y_test), len(y_test) + future_months), future_predictions, label='Future Predictions')\n",
    "    plt.title(f\"{attack_column} - Historical Data and Future Predictions\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f18171d-0e20-442f-a0ec-af3a1b6f660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikeras) (3.4.1)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikeras) (1.5.1)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (0.3.2)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->scikeras) (24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn>=1.4.2->scikeras) (1.14.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1.4.2->scikeras)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: joblib, scikeras\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.1\n",
      "    Uninstalling joblib-1.1.1:\n",
      "      Successfully uninstalled joblib-1.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.4.2 scikeras-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56281800-2663-40e2-8ac9-adff457ab6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib==1.1.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Installing collected packages: joblib\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-learn 1.5.1 requires joblib>=1.2.0, but you have joblib 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib==1.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5955f6d8-10a2-4fbf-999a-0d5e076ad9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moviepy 1.0.3 has requirement decorator<5.0,>=4.0.2, but you have decorator 5.1.1.\n",
      "scikit-learn 1.5.1 has requirement joblib>=1.2.0, but you have joblib 1.1.0.\n",
      "pydantic-settings 2.4.0 has requirement pydantic>=2.7.0, but you have pydantic 1.10.2.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f41c0-856b-445a-921d-8bd6b323432d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
