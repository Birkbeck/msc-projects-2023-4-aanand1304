{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from boruta import BorutaPy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import shap\n",
    "\n",
    "# SMAPE calculation function\n",
    "def smape(yTrue, yPred):\n",
    "    denominator = (np.abs(yTrue) + np.abs(yPred))\n",
    "    return np.mean(200 * np.abs(yPred - yTrue) / denominator)\n",
    "\n",
    "# Exponential Smoothing\n",
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return np.array(result)\n",
    "\n",
    "# Double Exponential Smoothing\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    level, trend = series[0], series[1] - series[0]\n",
    "    for n in range(1, len(series)):\n",
    "        value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return np.array(result)\n",
    "\n",
    "# Prepare multivariate data for LSTM input\n",
    "def prepare_multivariate_data(data, n_input, n_features):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_input):\n",
    "        X.append(data[i:(i + n_input), :])\n",
    "        y.append(data[i + n_input, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build the LSTM Model with Monte Carlo Dropout\n",
    "def build_mc_dropout_model(n_input, n_features, layer, unit, dropout_rate, activation='relu', optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_input, n_features)))\n",
    "    model.add(LSTM(unit[0], activation=activation, return_sequences=(layer > 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(1, layer):\n",
    "        model.add(LSTM(unit[min(i, len(unit)-1)], activation=activation, return_sequences=(i < layer-1)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Monte Carlo Dropout prediction function\n",
    "def mc_dropout_predict(model, X, n_iter=100):\n",
    "    predictions = np.array([model(X, training=True) for _ in range(n_iter)])\n",
    "    return predictions.mean(axis=0), predictions.std(axis=0)\n",
    "\n",
    "def compute_and_plot_shap(model, train_x, test_x, n_input, n_features, features, output_dir):\n",
    "    # Prepare the background dataset\n",
    "    n_samples = min(100, train_x.shape[0])\n",
    "    background = train_x[np.random.choice(train_x.shape[0], size=n_samples, replace=False)]\n",
    "    \n",
    "    # Initialize the SHAP GradientExplainer\n",
    "    explainer = shap.GradientExplainer(model, background)\n",
    "\n",
    "    # Compute SHAP values for the test set\n",
    "    shap_values = explainer.shap_values(test_x)\n",
    "\n",
    "    # Reshape shap_values and test_x for plotting\n",
    "    shap_values_reshaped = shap_values.reshape(shap_values.shape[0], -1)\n",
    "    test_x_reshaped = test_x.reshape(test_x.shape[0], -1)\n",
    "\n",
    "    # Create feature names for each timestep and feature\n",
    "    feature_names = []\n",
    "    for i in range(n_input):\n",
    "        for feature in features:  # Include the target variable here if necessary\n",
    "            feature_names.append(f'TimeStep_{i}_{feature}')\n",
    "\n",
    "    # Ensure the number of feature names matches the number of SHAP values\n",
    "    assert shap_values_reshaped.shape[1] == len(feature_names), \"Mismatch between SHAP values and feature names.\"\n",
    "\n",
    "    # Aggregate the absolute SHAP values to calculate feature importance\n",
    "    shap_values_abs = np.abs(shap_values_reshaped)\n",
    "    feature_importance = np.sum(shap_values_abs, axis=0)\n",
    "\n",
    "    # Create a dictionary of feature importance\n",
    "    feature_importance_dict = {feature_names[i]: feature_importance[i] for i in range(len(feature_names))}\n",
    "\n",
    "    # Sort features by importance\n",
    "    sorted_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the top N most important features (impact)\n",
    "    print(\"\\nTop 5 Features by SHAP Impact:\")\n",
    "    for feature, importance in sorted_importance[:5]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    # Plot the SHAP summary plot\n",
    "    shap.summary_plot(\n",
    "        shap_values_reshaped,\n",
    "        test_x_reshaped,\n",
    "        feature_names=feature_names,\n",
    "        show=False\n",
    "    )\n",
    "\n",
    "    # Save the plot\n",
    "    shap_plot_path = os.path.join(output_dir, 'shap_summary_plot.png')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(shap_plot_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"SHAP summary plot saved to {shap_plot_path}.\")\n",
    "\n",
    "# Evaluate the model with Monte Carlo Dropout\n",
    "def evaluate_mc_dropout_model(train, test, n_input, n_features, layer, unit, dropout_rate, scaler, epochs, features, output_dir, activation='relu', optimizer='adam'):\n",
    "    train_x, train_y = prepare_multivariate_data(train, n_input, n_features)\n",
    "    test_x, test_y = prepare_multivariate_data(test, n_input, n_features)\n",
    "    \n",
    "    model = build_mc_dropout_model(n_input, n_features, layer, unit, dropout_rate, activation=activation, optimizer=optimizer)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=8, validation_split=0.2, callbacks=[es], verbose=0)\n",
    "    \n",
    "    predictions_mean, predictions_std = mc_dropout_predict(model, test_x)\n",
    "    \n",
    "    # Inverse transform predictions and actual values\n",
    "    dummy_array = np.zeros((len(predictions_mean), n_features))\n",
    "    dummy_array[:, -1] = predictions_mean.flatten()\n",
    "    predictions_inv = scaler.inverse_transform(dummy_array)[:, -1]\n",
    "    \n",
    "    dummy_array[:, -1] = test_y\n",
    "    y_test_inv = scaler.inverse_transform(dummy_array)[:, -1]\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_bound = predictions_mean - 1.96 * predictions_std\n",
    "    upper_bound = predictions_mean + 1.96 * predictions_std\n",
    "    \n",
    "    dummy_array[:, -1] = lower_bound.flatten()\n",
    "    lower_bound_inv = scaler.inverse_transform(dummy_array)[:, -1]\n",
    "    \n",
    "    dummy_array[:, -1] = upper_bound.flatten()\n",
    "    upper_bound_inv = scaler.inverse_transform(dummy_array)[:, -1]\n",
    "    \n",
    "    smape_value = smape(y_test_inv, predictions_inv)\n",
    "    mae = mean_absolute_error(y_test_inv, predictions_inv)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inv, predictions_inv))\n",
    "    \n",
    "    # Compute and plot SHAP values\n",
    "    compute_and_plot_shap(model, train_x, test_x, n_input, n_features, features, output_dir)\n",
    "    \n",
    "    return smape_value, mae, rmse, predictions_inv, y_test_inv, lower_bound_inv, upper_bound_inv\n",
    "\n",
    "# Boruta feature selection\n",
    "def boruta_feature_selection(data, target_variable, features):\n",
    "    X = data[features].values\n",
    "    y = data[target_variable].values\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=42)\n",
    "    \n",
    "    boruta_selector.fit(X, y)\n",
    "    \n",
    "    selected_features = [features[i] for i in range(len(features)) if boruta_selector.support_[i]]\n",
    "    \n",
    "    if len(selected_features) >= 3:\n",
    "        best_features = selected_features[:3]\n",
    "    elif len(selected_features) >= 2:\n",
    "        best_features = selected_features[:2]\n",
    "    elif len(selected_features) >= 1:\n",
    "        best_features = selected_features[:1]\n",
    "    else:\n",
    "        best_features = []\n",
    "    \n",
    "    return best_features\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    data = pd.read_csv('FinalDataset.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%b-%y')\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    attacks = ['Phishing-ALL', 'Ransomware-ALL', 'Password Attack-ALL', 'SQL Injection-ALL',\n",
    "        'Account Hijacking-ALL', 'Defacement-ALL', 'Trojan-ALL', 'Vulnerability-ALL', 'Zero-day-ALL',\n",
    "        'Malware-ALL', 'Advanced persistent threat-ALL', 'XSS-ALL', 'Data Breach-ALL',\n",
    "        'Disinformation/Misinformation-ALL', 'Targeted Attack-ALL', 'Adware-ALL', 'Brute Force Attack-ALL',\n",
    "        'Malvertising-ALL', 'Backdoor-ALL', 'Botnet-ALL', 'Cryptojacking-ALL', 'Worms-ALL', 'Spyware-ALL', 'DDoS-ALL',]\n",
    "\n",
    "    features = [\n",
    "    # Economic Data\n",
    "    'GDP-ARE', 'GDP-AUS', 'GDP-AUT', 'GDP-BRA', 'GDP-CAN', 'GDP-CHE', 'GDP-CHN', 'GDP-DEU', 'GDP-EGY', 'GDP-ESP',\n",
    "    'GDP-FIN', 'GDP-FRA', 'GDP-GBR', 'GDP-IND', 'GDP-IRL', 'GDP-IRN', 'GDP-ISR', 'GDP-ITA', 'GDP-JPN', 'GDP-KOR',\n",
    "    'GDP-MEX', 'GDP-MYS', 'GDP-NLD', 'GDP-NOR', 'GDP-PAK', 'GDP-PRT', 'GDP-PSE', 'GDP-RUS', 'GDP-SAU', 'GDP-SWE',\n",
    "    'GDP-TUR', 'GDP-UKR', 'GDP-USA',\n",
    "    \n",
    "    # Social Media and Internet Data\n",
    "    'Internet Users (Millions)', 'Facebook Users (M)', 'Instagram Users (M)', 'Twitter Users (M)', 'LinkedIn Users (M)', 'Email Users (M)',\n",
    "    \n",
    "    # Holidays Data\n",
    "    'Holidays',\n",
    "    \n",
    "    # Mentions\n",
    "    'Mentions-DDoS', 'Mentions-Phishing', 'Mentions-Ransomware', 'Mentions-Password Attack', 'Mentions-SQL Injection',\n",
    "    'Mentions-Account Hijacking', 'Mentions-Defacement', 'Mentions-Trojan', 'Mentions-Vulnerability', 'Mentions-Zero-day',\n",
    "    'Mentions-Advanced persistent threat', 'Mentions-XSS', 'Mentions-Malware', 'Mentions-Data Breach', 'Mentions-Disinformation/Misinformation',\n",
    "    'Mentions-Targeted Attack', 'Mentions-Adware', 'Mentions-Brute Force Attack', 'Mentions-Malvertising', 'Mentions-Backdoor',\n",
    "    'Mentions-Botnet', 'Mentions-Cryptojacking', 'Mentions-Worms', 'Mentions-Spyware', 'Mentions-MITM', 'Mentions-DNS Spoofing',\n",
    "    'Mentions-Pegasus Spyware', 'Mentions-CoolWebSearch Spyware', 'Mentions-Gator GAIN Spyware', 'Mentions-180search Assistant Spyware',\n",
    "    'Mentions-Transponder vx2 Spyware', 'Mentions-WannaCry Ransomware', 'Mentions-Colonial Pipeline Ransomware', 'Mentions-Cryptolocker',\n",
    "    'Mentions-Dropper', 'Mentions-Wiper', 'Mentions-Pharming', 'Mentions-Insider Threat', 'Mentions-Drive-by', 'Mentions-Rootkit',\n",
    "    'Mentions-Adversarial Attack', 'Mentions-Data Poisoning', 'Mentions-Deepfake', 'Mentions-Deeplocker', 'Mentions-Supply Chain',\n",
    "    'Mentions-IoT Device Attack', 'Mentions-Keylogger', 'Mentions-DNS Tunneling', 'Mentions-Session Hijacking', 'Mentions-URL manipulation',\n",
    "    'Mentions-Unknown'\n",
    "\n",
    "    ]\n",
    "\n",
    "    output_dir = 'best_params_files_25M'\n",
    "    output_plot_dir = 'output_plot_25M'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(output_plot_dir, exist_ok=True)\n",
    "\n",
    "    for target_variable in attacks:\n",
    "        print(f\"\\nProcessing {target_variable}\")\n",
    "        \n",
    "        # Perform Boruta feature selection\n",
    "        selected_features = boruta_feature_selection(data, target_variable, features)\n",
    "        \n",
    "        if not selected_features:\n",
    "            print(f\"No significant features found for {target_variable}. Skipping this target.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Selected features for {target_variable}: {selected_features}\")\n",
    "        \n",
    "        # Include the target variable in the selected features\n",
    "        selected_features = selected_features + [target_variable]\n",
    "        \n",
    "        selected_data = data[selected_features].values\n",
    "        \n",
    "        best_smape = float('inf')\n",
    "        best_params = {}\n",
    "        \n",
    "        iterations = 100\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            print(f\"Iteration {iteration+1} out of {iterations}\")\n",
    "\n",
    "            # Apply smoothing to the target variable\n",
    "            alpha = random.uniform(0.1, 0.9)\n",
    "            beta = random.uniform(0.1, 0.9)\n",
    "            smoothing_method = random.choice(['none', 'exponential', 'double_exponential'])\n",
    "            data_copy = selected_data.copy()\n",
    "            if smoothing_method == 'exponential':\n",
    "                data_copy[:, -1] = exponential_smoothing(data_copy[:, -1], alpha)\n",
    "            elif smoothing_method == 'double_exponential':\n",
    "                data_copy[:, -1] = double_exponential_smoothing(data_copy[:, -1], alpha, beta)\n",
    "\n",
    "            # Plot data before and after smoothing on the same graph\n",
    "            original_target = selected_data[:, -1].copy()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(data.index, original_target, label='Original Data', color='blue')\n",
    "            plt.plot(data.index, data_copy[:, -1], label='Smoothed Data', color='green')\n",
    "            plt.title(f'{target_variable} - Original vs Smoothed Data')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Incident Count')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Scale the data\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(data_copy)\n",
    "\n",
    "            # Split the data\n",
    "            train_size = len(scaled_data) - 36\n",
    "            train, test = scaled_data[:train_size], scaled_data[train_size:]\n",
    "            test_dates = data.index[train_size:]\n",
    "\n",
    "            n_input = random.randint(1, 12)\n",
    "            n_epochs = random.choice([100, 200, 300, 400, 500])\n",
    "            layer = random.choice([1, 2, 3])\n",
    "            dropout_rate = random.uniform(0.1, 0.15)\n",
    "            activation_function = random.choice(['relu', 'tanh'])\n",
    "            optimizer_choice = random.choice(['adam', 'rmsprop'])\n",
    "\n",
    "            if layer == 1:\n",
    "                units = [random.choice([16, 32])]\n",
    "            elif layer == 2:\n",
    "                units = [random.choice([100, 200]), random.choice([50, 100])]\n",
    "            else:\n",
    "                units = [random.choice([200, 400]), random.choice([100, 200]), random.choice([50, 100])]\n",
    "\n",
    "            smape_value, mae, rmse, predictions, y_test_inv, lower_bound, upper_bound = evaluate_mc_dropout_model(\n",
    "                train, test, n_input, len(selected_features), layer, units, dropout_rate, scaler, n_epochs, selected_features, output_plot_dir,\n",
    "                activation=activation_function, optimizer=optimizer_choice\n",
    "            )\n",
    "\n",
    "            if smape_value < best_smape:\n",
    "                best_smape = smape_value\n",
    "                best_params = {\n",
    "                    'n_input': n_input,\n",
    "                    'n_epochs': n_epochs,\n",
    "                    'layer': layer,\n",
    "                    'units': units,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'smoothing_method': smoothing_method,\n",
    "                    'alpha': alpha,\n",
    "                    'beta': beta,\n",
    "                    'selected_features': selected_features\n",
    "                }\n",
    "                best_predictions = predictions\n",
    "                best_y_test_inv = y_test_inv\n",
    "                best_lower_bound = lower_bound\n",
    "                best_upper_bound = upper_bound\n",
    "\n",
    "        # Save the best parameters\n",
    "        file_path = os.path.join(output_dir, f'{target_variable}_best_params.json')\n",
    "        data_to_save = {\n",
    "            \"SMAPE\": best_smape,\n",
    "            \"Best Parameters\": best_params\n",
    "        }\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data_to_save, f)\n",
    "        print(f\"Best parameters for {target_variable} saved to {file_path}.\")\n",
    "        print(f\"Best SMAPE for {target_variable}: {best_smape}\")\n",
    "        print(f\"Best MAE: {mae}\")\n",
    "        print(f\"Best RMSE: {rmse}\")\n",
    "\n",
    "        # Plot the actual vs predicted values with confidence interval\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(test_dates[-len(best_y_test_inv):], best_y_test_inv, label='Actual', color='blue', linewidth=2)\n",
    "        plt.plot(test_dates[-len(best_predictions):], best_predictions, label='Predicted', color='red', linewidth=2)\n",
    "        plt.fill_between(test_dates[-len(best_predictions):], best_lower_bound, best_upper_bound, color='green', alpha=0.2, label='95% CI')\n",
    "        plt.title(f'{target_variable} (SMAPE: {best_smape:.2f}, Multivariate)')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Incident Count')\n",
    "        plt.legend()\n",
    "        plot_path = os.path.join(output_plot_dir, f'{target_variable}_actual_vs_predicted_mc_smoothed.png')\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"Plot for {target_variable} saved to {plot_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
