{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SMAPE calculation function\n",
    "def smape(yTrue, yPred):\n",
    "    \"\"\"SMAPE is used to evaluate the accuracy of the predictions.\"\"\"\n",
    "    denominator = (np.abs(yTrue) + np.abs(yPred))\n",
    "    smape_value = np.mean(2 * np.abs(yPred - yTrue) / np.where(denominator == 0, 1, denominator)) * 100\n",
    "    return smape_value\n",
    "\n",
    "# Exponential Smoothing\n",
    "def exponential_smoothing(series, alpha):\n",
    "    \"\"\"Apply exponential smoothing to a time series. reduces noise by smoothing out fluctuations.\"\"\"\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return np.array(result)\n",
    "\n",
    "# Double Exponential Smoothing\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    \"\"\"Apply double exponential smoothing to a time series, smoothing the level, and Beta is for smoothing the trend.\"\"\"\n",
    "    result = [series[0]]\n",
    "    level, trend = series[0], series[1] - series[0]\n",
    "    for n in range(1, len(series)):\n",
    "        value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return np.array(result)\n",
    "\n",
    "# Prepare univariate data for LSTM input\n",
    "def prepare_data(data, n_input):\n",
    "    \"\"\"Transform the time series data into sequences of input-output pairs for LSTM training \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_input):\n",
    "        X.append(data[i:(i + n_input)])\n",
    "        y.append(data[i + n_input])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build the LSTM Model\n",
    "def build_model(n_input, layer, unit, dropout_rate):\n",
    "    \"\"\"Build an LSTM model with Monte Carlo Dropout for uncertainty estimation\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_input, 1)))\n",
    "    \n",
    "    model.add(LSTM(unit[0], activation='relu', return_sequences=(layer > 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for i in range(1, layer):\n",
    "        model.add(LSTM(unit[min(i, len(unit)-1)], activation='relu', return_sequences=(i < layer-1)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Evaluate the model performance with confidence intervals\n",
    "def evaluate_model(train, test, n_input, layer, unit, dropout_rate, scaler, epochs):\n",
    "    train_x, train_y = prepare_data(train, n_input)\n",
    "    test_x, test_y = prepare_data(test, n_input)\n",
    "    \n",
    "    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n",
    "    test_x = test_x.reshape((test_x.shape[0], test_x.shape[1], 1))\n",
    "    \n",
    "    model = build_model(n_input, layer, unit, dropout_rate)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=8, validation_split=0.2, callbacks=[es], verbose=0)\n",
    "    \n",
    "    n_iterations = 20\n",
    "    predictions_mc = np.array([model(test_x, training=True) for _ in range(n_iterations)])\n",
    "    \n",
    "    predictions_mean = np.mean(predictions_mc, axis=0).reshape(-1, 1)\n",
    "    predictions_std = np.std(predictions_mc, axis=0).reshape(-1, 1)\n",
    "    \n",
    "    lower_bound = predictions_mean - 1.96 * predictions_std\n",
    "    upper_bound = predictions_mean + 1.96 * predictions_std\n",
    "    \n",
    "    predictions_mean = scaler.inverse_transform(predictions_mean)\n",
    "    lower_bound = scaler.inverse_transform(lower_bound)\n",
    "    upper_bound = scaler.inverse_transform(upper_bound)\n",
    "    y_test_inv = scaler.inverse_transform(test_y.reshape(-1, 1))\n",
    "    \n",
    "    smape_value = smape(y_test_inv, predictions_mean)\n",
    "    mae = mean_absolute_error(y_test_inv, predictions_mean)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inv, predictions_mean))\n",
    "    \n",
    "    return smape_value, mae, rmse, predictions_mean, y_test_inv, lower_bound, upper_bound\n",
    "\n",
    "# Main execution for univariate forecasting\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    data = pd.read_csv('FinalDataset.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%b-%y')\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    attacks = ['DDoS-ALL', 'Phishing-ALL', 'Ransomware-ALL', 'Password Attack-ALL', 'SQL Injection-ALL', 'Account Hijacking-ALL', \n",
    "            'Defacement-ALL', 'Trojan-ALL', 'Vulnerability-ALL', 'Zero-day-ALL', 'Malware-ALL', 'Advanced persistent threat-ALL', \n",
    "            'XSS-ALL', 'Data Breach-ALL', 'Disinformation/Misinformation-ALL', 'Targeted Attack-ALL','Adware-ALL',\n",
    "            'Brute Force Attack-ALL', 'Malvertising-ALL', 'Backdoor-ALL', 'Botnet-ALL', 'Cryptojacking-ALL',\n",
    "            'Worms-ALL', 'Spyware-ALL']\n",
    "\n",
    "    # Create directories for saving best parameters and plots\n",
    "    output_param_dir = 'univariateparam25'\n",
    "    output_plot_dir = 'univariateplot25'\n",
    "    os.makedirs(output_param_dir, exist_ok=True)\n",
    "    os.makedirs(output_plot_dir, exist_ok=True)\n",
    "\n",
    "    # Hyperparameters for random search\n",
    "    alphas = [0.05, 0.2, 0.5, 0.7, 1]\n",
    "    betas = [0.3, 0.5, 0.7, 1]\n",
    "    lags = [1, 3, 6, 12]\n",
    "    epochs_list = [100, 200, 300, 400, 500]\n",
    "    dropout_rates = [0.1, 0.15, 0.2]\n",
    "\n",
    "    # Loop through each attack type (univariate forecast for each)\n",
    "    for attack in attacks:\n",
    "        print(f\"Processing attack: {attack}\")\n",
    "\n",
    "        # Select the data for the current attack (only one column for univariate)\n",
    "        attack_data = data[attack].values.reshape(-1, 1)\n",
    "\n",
    "        best_smape = float('inf')\n",
    "        best_params = {}\n",
    "        best_model = None\n",
    "\n",
    "        iterations = 100\n",
    "\n",
    "        # Iterate through random hyperparameters to find the best configuration\n",
    "        for iteration in range(iterations):\n",
    "            print(f\"Iteration {iteration + 1} out of {iterations}\")\n",
    "\n",
    "            # Randomly generate hyperparameters\n",
    "            alpha = random.choice(alphas)\n",
    "            beta = random.choice(betas)\n",
    "            n_input = random.choice(lags)\n",
    "            n_epochs = random.choice(epochs_list)\n",
    "            layer = random.choice([1, 2, 3])\n",
    "            dropout_rate = random.choice(dropout_rates)\n",
    "\n",
    "\n",
    "            # Apply smoothing\n",
    "            if alpha != 1 and beta != 1:\n",
    "                smoothed_data = double_exponential_smoothing(attack_data.flatten(), alpha, beta).reshape(-1, 1)\n",
    "            elif alpha != 1 and beta == 1:\n",
    "                smoothed_data = exponential_smoothing(attack_data.flatten(), alpha).reshape(-1, 1)\n",
    "            else:\n",
    "                smoothed_data = attack_data\n",
    "                \n",
    "           # Plot data before and after smoothing on the same graph\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(data.index, attack_data.flatten(), label='Original Data', color='blue')\n",
    "            plt.plot(data.index, smoothed_data.flatten(), label='Smoothed Data', color='green')\n",
    "            plt.title(f'{attack} - Original vs Smoothed Data')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Incident Count')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Scale the smoothed data\n",
    "            scaler = RobustScaler()\n",
    "            scaled_data = scaler.fit_transform(smoothed_data)\n",
    "\n",
    "            # Split into train and test sets\n",
    "            train_size = len(scaled_data) - 36\n",
    "            train, test = scaled_data[:train_size], scaled_data[train_size:]\n",
    "            test_dates = data.index[train_size:]\n",
    "\n",
    "            if layer == 1:\n",
    "                units = [random.choice([16, 32])]\n",
    "            elif layer == 2:\n",
    "                units = [random.choice([100, 200]), random.choice([50, 100])]\n",
    "            else:\n",
    "                units = [random.choice([200, 400]), random.choice([100, 200]), random.choice([50, 100])]\n",
    "\n",
    "            # Evaluate the model with the current hyperparameters\n",
    "            smape_value, mae, rmse, predictions_mean, y_test_inv, lower_bound, upper_bound = evaluate_model(\n",
    "                train, test, n_input, layer, units, dropout_rate, scaler, n_epochs\n",
    "            )\n",
    "\n",
    "            # Save the best model parameters\n",
    "            if smape_value < best_smape:\n",
    "                best_smape = smape_value\n",
    "                best_params = {\n",
    "                    'alpha': alpha, 'beta': beta, 'n_input': n_input, 'n_epochs': n_epochs,\n",
    "                    'layer': layer, 'units': units, 'dropout_rate': dropout_rate\n",
    "                }\n",
    "                best_predictions_mean = predictions_mean\n",
    "                best_y_test_inv = y_test_inv\n",
    "                best_lower_bound = lower_bound\n",
    "                best_upper_bound = upper_bound\n",
    "\n",
    "        # Save the best parameters for the current attack in a separate JSON file\n",
    "        file_path = os.path.join(output_param_dir, f'{attack}_best_params.json')\n",
    "        data_to_save = {\n",
    "            \"SMAPE\": best_smape,\n",
    "            \"Best Parameters\": best_params\n",
    "        }\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data_to_save, f)\n",
    "\n",
    "        # Adjust test_dates to match the length of best_y_test_inv\n",
    "        test_dates = test_dates[-len(best_y_test_inv):]\n",
    "\n",
    "        print(f\"Best parameters for {attack} saved to {file_path}.\")\n",
    "        print(f\"Best SMAPE for {attack}: {best_smape}\")\n",
    "        print(f\"Best MAE: {mae}\")\n",
    "        print(f\"Best RMSE: {rmse}\")\n",
    "\n",
    "        # Plot the actual vs predicted values and save the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot actual values and predicted mean values\n",
    "        plt.plot(test_dates, best_y_test_inv, label='Actual', color='blue', linewidth=2)\n",
    "        plt.plot(test_dates, best_predictions_mean, label='Predicted Mean', color='red', linewidth=2)\n",
    "\n",
    "        # Plot the confidence intervals (upper and lower bounds)\n",
    "        plt.fill_between(\n",
    "            test_dates,\n",
    "            best_lower_bound.flatten(),   # Lower bound of the 95% confidence interval\n",
    "            best_upper_bound.flatten(),   # Upper bound of the 95% confidence interval\n",
    "            color='green', alpha=0.3, label='95% Confidence Interval'\n",
    "        )\n",
    "\n",
    "        # Add titles, labels, and legend\n",
    "        plt.title(f'{attack} (SMAPE: {best_smape:.2f}, Univariate)')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Incident Count')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(output_plot_dir, f'{attack}_actual_vs_predicted.png')\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Plot for {attack} saved to {plot_path}.\")\n",
    "\n",
    "print(\"Best parameters for all attacks have been saved individually.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
