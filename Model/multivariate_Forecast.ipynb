{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from boruta import BorutaPy\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import os\n",
    "import matplotlib.dates as mdates\n",
    "import random\n",
    "\n",
    "# SMAPE calculation function\n",
    "def smape(yTrue, yPred):\n",
    "    denominator = (np.abs(yTrue) + np.abs(yPred))\n",
    "    return np.mean(200 * np.abs(yPred - yTrue) / denominator)\n",
    "\n",
    "# Exponential Smoothing\n",
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return np.array(result)\n",
    "\n",
    "# Double Exponential Smoothing\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    level, trend = series[0], series[1] - series[0]\n",
    "    for n in range(1, len(series)):\n",
    "        value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return np.array(result)\n",
    "\n",
    "# Prepare multivariate data for LSTM input\n",
    "def prepare_multivariate_data(data, n_input, n_features):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_input):\n",
    "        X.append(data[i:(i + n_input), :])\n",
    "        y.append(data[i + n_input, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build the LSTM Model with Monte Carlo Dropout\n",
    "def build_mc_dropout_model(n_input, n_features, layer, unit, dropout_rate, activation='relu', optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(n_input, n_features)))\n",
    "    model.add(LSTM(unit[0], activation=activation, return_sequences=(layer > 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for i in range(1, layer):\n",
    "        model.add(LSTM(unit[min(i, len(unit)-1)], activation=activation, return_sequences=(i < layer-1)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Monte Carlo Dropout prediction function\n",
    "def mc_dropout_predict(model, X, n_iter=100):\n",
    "    predictions = np.array([model(X, training=True) for _ in range(n_iter)])\n",
    "    return predictions.mean(axis=0), predictions.std(axis=0)\n",
    "\n",
    "# Function to generate future dates\n",
    "def generate_future_dates(start_date, periods):\n",
    "    return [start_date + timedelta(days=i*30) for i in range(periods)] \n",
    "\n",
    "# Boruta feature selection\n",
    "def boruta_feature_selection(data, target_variable, features):\n",
    "    X = data[features].values\n",
    "    y = data[target_variable].values\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=42)\n",
    "    \n",
    "    boruta_selector.fit(X, y)\n",
    "    \n",
    "    selected_features = [features[i] for i in range(len(features)) if boruta_selector.support_[i]]\n",
    "    \n",
    "    if len(selected_features) >= 3:\n",
    "        best_features = selected_features[:3]\n",
    "    elif len(selected_features) >= 2:\n",
    "        best_features = selected_features[:2]\n",
    "    elif len(selected_features) >= 1:\n",
    "        best_features = selected_features[:1]\n",
    "    else:\n",
    "        best_features = []\n",
    "    \n",
    "    return best_features\n",
    "\n",
    "# Function to run seed-based forecasting and plot results\n",
    "def run_seed_forecasting(target_variable, selected_data, scaler, model, forecast_horizon, data, last_date, output_plot_dir, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "    last_sequence = selected_data[-model.input_shape[1]:]\n",
    "    forecasts, lower_bounds, upper_bounds = [], [], []\n",
    "    \n",
    "    for _ in range(forecast_horizon):\n",
    "        next_prediction_mean, next_prediction_std = mc_dropout_predict(model, last_sequence.reshape(1, model.input_shape[1], model.input_shape[2]))\n",
    "        forecasts.append(next_prediction_mean[0, 0])\n",
    "        \n",
    "        lower_bound_scaled = next_prediction_mean[0, 0] - 1.96 * next_prediction_std[0, 0]\n",
    "        upper_bound_scaled = next_prediction_mean[0, 0] + 1.96 * next_prediction_std[0, 0]\n",
    "        \n",
    "        lower_bounds.append(lower_bound_scaled)\n",
    "        upper_bounds.append(upper_bound_scaled)\n",
    "        \n",
    "        last_sequence = np.roll(last_sequence, -1, axis=0)\n",
    "        last_sequence[-1, -1] = next_prediction_mean[0, 0]\n",
    "    \n",
    "    dummy_array = np.zeros((len(forecasts), model.input_shape[2]))\n",
    "    dummy_array[:, -1] = forecasts\n",
    "    forecasts_inv = scaler.inverse_transform(dummy_array)[:, -1]\n",
    "    \n",
    "    dummy_array[:, -1] = lower_bounds\n",
    "    lower_bounds_inv = scaler.inverse_transform(dummy_array)[:, -1]\n",
    "    \n",
    "    dummy_array[:, -1] = upper_bounds\n",
    "    upper_bounds_inv = scaler.inverse_transform(dummy_array)[:, -1]\n",
    "    \n",
    "    future_dates = generate_future_dates(last_date, forecast_horizon)\n",
    "    future_dates = [last_date] + future_dates\n",
    "    \n",
    "    seamless_forecast = np.insert(forecasts_inv, 0, data[target_variable].iloc[-1])\n",
    "    lower_bounds_inv = np.insert(lower_bounds_inv, 0, data[target_variable].iloc[-1])\n",
    "    upper_bounds_inv = np.insert(upper_bounds_inv, 0, data[target_variable].iloc[-1])\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(data.index, data[target_variable], label='Data', color='blue', linestyle='-')\n",
    "    plt.plot(future_dates, seamless_forecast, label=f'Prediction (Seed {seed})' if seed is not None else 'Prediction', color='red', linestyle='--')\n",
    "    plt.fill_between(future_dates, lower_bounds_inv, upper_bounds_inv, color='green', alpha=0.2, label='95% Confidence')\n",
    "    \n",
    "    plt.title(f'{target_variable} - (M) - Seed {seed}' if seed is not None else f'{target_variable} - (M)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Incident Count')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    \n",
    "    plot_name = f'{target_variable}_forecast_seed_{seed}.png' if seed is not None else f'{target_variable}_forecast.png'\n",
    "    plot_path = os.path.join(output_plot_dir, plot_name)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Forecast plot for {target_variable} with seed {seed} saved to {plot_path}.\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    data = pd.read_csv('FinalDataset.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%b-%y')\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    attacks = ['Phishing-ALL', 'Ransomware-ALL', 'Password Attack-ALL', 'SQL Injection-ALL',\n",
    "        'Account Hijacking-ALL', 'Defacement-ALL', 'Trojan-ALL', 'Vulnerability-ALL', 'Zero-day-ALL',\n",
    "        'Malware-ALL', 'Advanced persistent threat-ALL', 'XSS-ALL', 'Data Breach-ALL',\n",
    "        'Disinformation/Misinformation-ALL', 'Targeted Attack-ALL', 'Adware-ALL', 'Brute Force Attack-ALL',\n",
    "        'Malvertising-ALL', 'Backdoor-ALL', 'Botnet-ALL', 'Cryptojacking-ALL', 'Worms-ALL', 'Spyware-ALL', 'DDoS-ALL',]\n",
    "\n",
    "    features = [\n",
    "    # Economic Data\n",
    "    'GDP-ARE', 'GDP-AUS', 'GDP-AUT', 'GDP-BRA', 'GDP-CAN', 'GDP-CHE', 'GDP-CHN', 'GDP-DEU', 'GDP-EGY', 'GDP-ESP',\n",
    "    'GDP-FIN', 'GDP-FRA', 'GDP-GBR', 'GDP-IND', 'GDP-IRL', 'GDP-IRN', 'GDP-ISR', 'GDP-ITA', 'GDP-JPN', 'GDP-KOR',\n",
    "    'GDP-MEX', 'GDP-MYS', 'GDP-NLD', 'GDP-NOR', 'GDP-PAK', 'GDP-PRT', 'GDP-PSE', 'GDP-RUS', 'GDP-SAU', 'GDP-SWE',\n",
    "    'GDP-TUR', 'GDP-UKR', 'GDP-USA',\n",
    "    \n",
    "    # Social Media and Internet Data\n",
    "    'Internet Users (Millions)', 'Facebook Users (M)', 'Instagram Users (M)', 'Twitter Users (M)', 'LinkedIn Users (M)', 'Email Users (M)',\n",
    "    \n",
    "    # Holidays Data\n",
    "    'Holidays',\n",
    "    \n",
    "    # Mentions\n",
    "    'Mentions-DDoS', 'Mentions-Phishing', 'Mentions-Ransomware', 'Mentions-Password Attack', 'Mentions-SQL Injection',\n",
    "    'Mentions-Account Hijacking', 'Mentions-Defacement', 'Mentions-Trojan', 'Mentions-Vulnerability', 'Mentions-Zero-day',\n",
    "    'Mentions-Advanced persistent threat', 'Mentions-XSS', 'Mentions-Malware', 'Mentions-Data Breach', 'Mentions-Disinformation/Misinformation',\n",
    "    'Mentions-Targeted Attack', 'Mentions-Adware', 'Mentions-Brute Force Attack', 'Mentions-Malvertising', 'Mentions-Backdoor',\n",
    "    'Mentions-Botnet', 'Mentions-Cryptojacking', 'Mentions-Worms', 'Mentions-Spyware', 'Mentions-MITM', 'Mentions-DNS Spoofing',\n",
    "    'Mentions-Pegasus Spyware', 'Mentions-CoolWebSearch Spyware', 'Mentions-Gator GAIN Spyware', 'Mentions-180search Assistant Spyware',\n",
    "    'Mentions-Transponder vx2 Spyware', 'Mentions-WannaCry Ransomware', 'Mentions-Colonial Pipeline Ransomware', 'Mentions-Cryptolocker',\n",
    "    'Mentions-Dropper', 'Mentions-Wiper', 'Mentions-Pharming', 'Mentions-Insider Threat', 'Mentions-Drive-by', 'Mentions-Rootkit',\n",
    "    'Mentions-Adversarial Attack', 'Mentions-Data Poisoning', 'Mentions-Deepfake', 'Mentions-Deeplocker', 'Mentions-Supply Chain',\n",
    "    'Mentions-IoT Device Attack', 'Mentions-Keylogger', 'Mentions-DNS Tunneling', 'Mentions-Session Hijacking', 'Mentions-URL manipulation',\n",
    "    'Mentions-Unknown'\n",
    "\n",
    "    ]\n",
    "\n",
    "    output_plot_dir = 'Forecast_plot_Multivariate'\n",
    "    os.makedirs(output_plot_dir, exist_ok=True)\n",
    "\n",
    "    for target_variable in attacks:\n",
    "        print(f\"\\nProcessing {target_variable}\")\n",
    "        \n",
    "        # Perform Boruta feature selection\n",
    "        selected_features = boruta_feature_selection(data, target_variable, features)\n",
    "        \n",
    "        if not selected_features:\n",
    "            print(f\"No significant features found for {target_variable}. Skipping this target.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Selected features for {target_variable}: {selected_features}\")\n",
    "        \n",
    "        # Include the target variable in the selected features\n",
    "        selected_features = selected_features + [target_variable]\n",
    "        \n",
    "        selected_data = data[selected_features].values\n",
    "        \n",
    "        # Load best parameters\n",
    "        with open(f'best_params_files_25M/{target_variable}_best_params.json', 'r') as f:\n",
    "            best_params = json.load(f)['Best Parameters']\n",
    "\n",
    "        # Apply smoothing to the target variable\n",
    "        if best_params['smoothing_method'] == 'exponential':\n",
    "            selected_data[:, -1] = exponential_smoothing(selected_data[:, -1], best_params['alpha'])\n",
    "        elif best_params['smoothing_method'] == 'double_exponential':\n",
    "            selected_data[:, -1] = double_exponential_smoothing(selected_data[:, -1], best_params['alpha'], best_params['beta'])\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(selected_data)\n",
    "        \n",
    "        # Prepare data for LSTM\n",
    "        n_input = best_params['n_input']\n",
    "        n_features = len(selected_features)\n",
    "        X, y = prepare_multivariate_data(scaled_data, n_input, n_features)\n",
    "\n",
    "        # Train on Full Data - n_input\n",
    "        X_train = X[:-n_input]\n",
    "        y_train = y[:-n_input]\n",
    "        \n",
    "        # Build and train the model on the complete dataset\n",
    "        model = build_mc_dropout_model(n_input, n_features, best_params['layer'], best_params['units'], best_params['dropout_rate'])\n",
    "        model.fit(X_train, y_train, epochs=best_params['n_epochs'], batch_size=32, verbose=0)\n",
    "        \n",
    "        # Generate forecast for the next 3 years (36 months)\n",
    "        forecast_horizon = 36\n",
    "        last_date = data.index[-1]\n",
    "        \n",
    "        # Run original forecast (no seed)\n",
    "        \n",
    "        run_seed_forecasting(target_variable, scaled_data, scaler, model, forecast_horizon, data, last_date, output_plot_dir)\n",
    "        \n",
    "        # Run seed-based forecasting\n",
    "        for seed in [1, 2, 3]:\n",
    "            run_seed_forecasting(target_variable, scaled_data, scaler, model, forecast_horizon, data, last_date, output_plot_dir, seed=seed)\n",
    "\n",
    "    print(\"Forecasting completed for all attack types.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
